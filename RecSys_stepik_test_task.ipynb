{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from surprise import Reader, Dataset, SVD, accuracy\n",
    "from surprise.model_selection.validation import cross_validate\n",
    "\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/dataset.csv')\n",
    "data = data.sort_values(['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use here Collaborative filtering using SurPRISE library in Python. https://surprise.readthedocs.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dataset\n",
    "reader = Reader()\n",
    "data = data.drop('timestamp', axis=1)\n",
    "data = Dataset.load_from_df(data, reader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make train (80%) and test(20%) datasets using function from surprise. As this task is designed we dont need to shuffle. I think the author of the task wanted us to make predictions based on older observations. As we can see in my \"draft\" notebook we have data for the train mostly from 1997, for the test from 1998. Our predictions will be more life-oriented, since we always have older observations and have to build predictions for the current ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data does not have that many features to apply some neural networks or more sophisticated algorithms. That is why i suggest to try Collaborative filtering here. Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users. So, we do have users, items and some ratings (which in fact how they rate items). For avoiding problems such as _scalability_ and _sparsity_ I am offering to use SVD (Single Value Decomposition). Like I said before, beautiful framework SuPRISE has these algorithms to use (and many more!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our model with 10 factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x11e0420f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = SVD(n_factors=10, random_state=17)\n",
    "# fit\n",
    "svd.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build predictions for our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20001\n"
     ]
    }
   ],
   "source": [
    "predictions = svd.test(test)\n",
    "\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our model now has ability to build the estimations for the rating based on given user and item. In our case we need to recommend our users most relevant items. I suggest to save best (top/first) n items, that the user would rate if he had to do that. So when we build a prediction we just sort our itmes by the estimated rating for this particular item and user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_recs(pred, n):\n",
    "    first_n = defaultdict(list)\n",
    "    # loop\n",
    "    for user_id, item_id, x, estimation, _ in predictions:\n",
    "        first_n[user_id].append((item_id, estimation))\n",
    "    \n",
    "    # sort by the estimation rating for each user\n",
    "    for user_id, est_ratings in first_n.items():\n",
    "        # sort by rating\n",
    "        est_ratings.sort(key=lambda a: a[1], reverse=True)\n",
    "        first_n[user_id] = est_ratings[:n]\n",
    "\n",
    "    return first_n    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user, first_n):\n",
    "    return [x[0] for x in first_n[user]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get first n items for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_10 = first_recs(predictions, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function was given by the author, but we still need to understand what is under the hood. Average precision function reward our model for the continious right precisions. If we predict first m items correctly we will have the maximum possible value for this m items. But if we miss some l-th < m, next right predictions will be rewarded less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(actual, recommended, k=30):\n",
    "    ap_sum = 0\n",
    "    hits = 0\n",
    "    for i in range(k):\n",
    "        product_id = recommended[i] if i < len(recommended) else None\n",
    "        if product_id is not None and product_id in actual:\n",
    "            hits += 1\n",
    "            ap_sum += hits / (i + 1)\n",
    "    return ap_sum / k\n",
    "\n",
    "\n",
    "def normalized_average_precision(actual, recommended, k=30):\n",
    "    actual = set(actual)\n",
    "    if len(actual) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    ap = average_precision(actual, recommended, k=k)\n",
    "    ap_ideal = average_precision(actual, list(actual)[:k], k=k)\n",
    "    return ap / ap_ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's calculate the score and see if our solution worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_score = pd.read_csv('data/dataset.csv')\n",
    "data_for_score = data_for_score.sort_values(['timestamp'])\n",
    "\n",
    "test_for_score = data_for_score[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [00:00<00:00, 1046.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5061151854702257"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for user in tqdm(test_for_score['user_id'].unique()):\n",
    "    actual = list(test_for_score[test_for_score['user_id'] == user]['item_id'])\n",
    "    recommended = recommend(user, first_10)\n",
    "    \n",
    "    scores.append(normalized_average_precision(actual, recommended))\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got 0.506 > 0.1. Awesome result, but i think we can upgrade the solution and make it better. Let's discuss it in next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My suggestions for model improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, i should note that improvement is not only about getting high value of the metric. Our model could give high score on some data, but low score for other data. So we need to build _stable_ model.\n",
    "\n",
    "1. SVD has some parametres that in theory can improve the result\n",
    "    1.1 For this I can offer build a Grid Search for finding the best subset of the model selection.\n",
    "    1.2 Learning rates for variables are also crucial in this algorthms and should be optimized\n",
    "    \n",
    "2. In my \"draft\" notebook i did excellent work on extracting information from timestamp variable. I am sure this information could be useful for more sophisticated algorithms (like stacking different models where we can use those features)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
